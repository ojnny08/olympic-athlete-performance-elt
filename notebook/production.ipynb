{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60573d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 23:31:42 WARN FileSystem: Failed to initialize filesystem hdfs://namenode:9000: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "26/02/21 23:31:42 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /data/raw/athletes.csv.\n",
      "java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n",
      "\tat org.apache.hadoop.security.SecurityUtil.buildTokenService(SecurityUtil.java:479)\n",
      "\tat org.apache.hadoop.hdfs.NameNodeProxiesClient.createProxyWithClientProtocol(NameNodeProxiesClient.java:134)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:370)\n",
      "\tat org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:309)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.initDFSClient(DistributedFileSystem.java:205)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:190)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:289)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:541)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.sinks.FileStreamSink$.hasMetadata(FileStreamSink.scala:57)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:384)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:248)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:245)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:237)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:237)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:343)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:224)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:339)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:289)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:207)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:236)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:84)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:322)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:717)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:330)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:329)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:139)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1392)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:150)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:90)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:114)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:112)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:108)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:57)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:258)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.net.UnknownHostException: namenode\n",
      "\t... 90 more\n",
      "26/02/21 23:31:42 WARN FileSystem: Failed to initialize filesystem hdfs://namenode:9000: java.lang.IllegalArgumentException: java.net.UnknownHostException: namenode\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "java.net.UnknownHostException: namenode",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functions \u001b[38;5;28;01mas\u001b[39;00m F\n\u001b[32m      4\u001b[39m spark = SparkSession.builder \\\n\u001b[32m      5\u001b[39m     .appName(\u001b[33m\"\u001b[39m\u001b[33mOlympicsExtraction\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m      6\u001b[39m     .master(\u001b[33m\"\u001b[39m\u001b[33mlocal[4]\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     .config(\u001b[33m\"\u001b[39m\u001b[33mspark.sql.repl.eagerEval.maxNumRows\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m5\u001b[39m) \\\n\u001b[32m     13\u001b[39m     .getOrCreate()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m athletes = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/data/raw/athletes.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m results = spark.read.csv(\u001b[33m\"\u001b[39m\u001b[33m/data/raw/results.csv\u001b[39m\u001b[33m\"\u001b[39m, header=\u001b[38;5;28;01mTrue\u001b[39;00m, inferSchema=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m athletes\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode/Olympic_Athletes/env/lib/python3.14/site-packages/pyspark/sql/readwriter.py:838\u001b[39m, in \u001b[36mDataFrameReader.csv\u001b[39m\u001b[34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[39m\n\u001b[32m    836\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    837\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode/Olympic_Athletes/env/lib/python3.14/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/vscode/Olympic_Athletes/env/lib/python3.14/site-packages/pyspark/errors/exceptions/captured.py:269\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    265\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    267\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    268\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: java.net.UnknownHostException: namenode"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OlympicsExtraction\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"100s\") \\\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
    "    .config(\"spark.sql.repl.eagerEval.maxNumRows\", 5) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "athletes = spark.read.csv(\"/data/raw/athletes.csv\", header=True, inferSchema=True)\n",
    "results = spark.read.csv(\"/data/raw/results.csv\", header=True, inferSchema=True)\n",
    "\n",
    "athletes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8451d8ce",
   "metadata": {},
   "source": [
    "## Now use pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d2b07",
   "metadata": {},
   "source": [
    "- Name, height, weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9563c8e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Roles</th><th>Sex</th><th>Full name</th><th>Used name</th><th>Born</th><th>Died</th><th>NOC</th><th>athlete_id</th><th>Measurements</th><th>Affiliations</th><th>Nick/petnames</th><th>Title(s)</th><th>Other names</th><th>Nationality</th><th>Original name</th><th>Name order</th><th>Name</th><th>Height_cm</th><th>Weight_kg</th><th>Born_year</th><th>Death_year</th><th>Born_date</th><th>Death_date</th><th>Birth_location</th><th>City</th><th>Region</th><th>Country</th></tr>\n",
       "<tr><td>Competed in Olymp...</td><td>Male</td><td>&quot;Fran&ccedil;ois Joseph ...</td><td>Jean-Fran&ccedil;ois&bull;Bla...</td><td>12 December 1886 ...</td><td>2 October 1960 in...</td><td>France</td><td>1</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Jean-Fran&ccedil;ois Bla...</td><td>NULL</td><td>NULL</td><td>1886</td><td>1960</td><td>12 December 1886</td><td>2 October 1960</td><td>Bordeaux, Gironde...</td><td>Bordeaux</td><td>Gironde</td><td>FRA</td></tr>\n",
       "<tr><td>Competed in Olymp...</td><td>Male</td><td>Arnaud Benjamin&bull;B...</td><td>Arnaud&bull;Boetsch</td><td>1 April 1969 in M...</td><td>NULL</td><td>France</td><td>2</td><td>183 cm / 76 kg</td><td>Racing Club de Fr...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Arnaud Boetsch</td><td>183</td><td>76</td><td>1969</td><td>NULL</td><td>1 April 1969</td><td>NULL</td><td>Meulan, Yvelines ...</td><td>Meulan</td><td>Yvelines</td><td>FRA</td></tr>\n",
       "<tr><td>Competed in Olymp...</td><td>Male</td><td>Jean Laurent Robe...</td><td>Jean&bull;Borotra</td><td>13 August 1898 in...</td><td>17 July 1994 in A...</td><td>France</td><td>3</td><td>183 cm / 76 kg</td><td>TCP, Paris (FRA)</td><td>Le Basque Bondiss...</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Jean Borotra</td><td>183</td><td>76</td><td>1898</td><td>1994</td><td>13 August 1898</td><td>17 July 1994</td><td>Biarritz, Pyr&eacute;n&eacute;e...</td><td>Biarritz</td><td>NULL</td><td>FRA</td></tr>\n",
       "<tr><td>Competed in Olymp...</td><td>Male</td><td>Jacques Marie Sta...</td><td>Jacques&bull;Brugnon</td><td>11 May 1895 in Pa...</td><td>20 March 1978 in ...</td><td>France</td><td>4</td><td>168 cm / 64 kg</td><td>Sporting club de ...</td><td>Toto</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Jacques Brugnon</td><td>168</td><td>64</td><td>1895</td><td>1978</td><td>11 May 1895</td><td>20 March 1978</td><td>Paris VIIIe, Pari...</td><td>Paris VIIIe</td><td>Paris</td><td>FRA</td></tr>\n",
       "<tr><td>Competed in Olymp...</td><td>Male</td><td>Henry Albert&bull;Canet</td><td>Albert&bull;Canet</td><td>17 April 1878 in ...</td><td>25 July 1930 in P...</td><td>France</td><td>5</td><td>NULL</td><td>TCP, Paris (FRA)</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Albert Canet</td><td>NULL</td><td>NULL</td><td>1878</td><td>1930</td><td>17 April 1878</td><td>25 July 1930</td><td>Wandsworth, Engla...</td><td>Wandsworth</td><td>England</td><td>GBR</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+--------------------+----+--------------------+--------------------+--------------------+--------------------+------+----------+--------------+--------------------+--------------------+--------+-----------+-----------+-------------+----------+--------------------+---------+---------+---------+----------+----------------+--------------+--------------------+-----------+--------+-------+\n",
       "|               Roles| Sex|           Full name|           Used name|                Born|                Died|   NOC|athlete_id|  Measurements|        Affiliations|       Nick/petnames|Title(s)|Other names|Nationality|Original name|Name order|                Name|Height_cm|Weight_kg|Born_year|Death_year|       Born_date|    Death_date|      Birth_location|       City|  Region|Country|\n",
       "+--------------------+----+--------------------+--------------------+--------------------+--------------------+------+----------+--------------+--------------------+--------------------+--------+-----------+-----------+-------------+----------+--------------------+---------+---------+---------+----------+----------------+--------------+--------------------+-----------+--------+-------+\n",
       "|Competed in Olymp...|Male|\"François Joseph ...|Jean-François•Bla...|12 December 1886 ...|2 October 1960 in...|France|         1|          NULL|                NULL|                NULL|    NULL|       NULL|       NULL|         NULL|      NULL|Jean-François Bla...|     NULL|     NULL|     1886|      1960|12 December 1886|2 October 1960|Bordeaux, Gironde...|   Bordeaux| Gironde|    FRA|\n",
       "|Competed in Olymp...|Male|Arnaud Benjamin•B...|      Arnaud•Boetsch|1 April 1969 in M...|                NULL|France|         2|183 cm / 76 kg|Racing Club de Fr...|                NULL|    NULL|       NULL|       NULL|         NULL|      NULL|      Arnaud Boetsch|      183|       76|     1969|      NULL|    1 April 1969|          NULL|Meulan, Yvelines ...|     Meulan|Yvelines|    FRA|\n",
       "|Competed in Olymp...|Male|Jean Laurent Robe...|        Jean•Borotra|13 August 1898 in...|17 July 1994 in A...|France|         3|183 cm / 76 kg|    TCP, Paris (FRA)|Le Basque Bondiss...|    NULL|       NULL|       NULL|         NULL|      NULL|        Jean Borotra|      183|       76|     1898|      1994|  13 August 1898|  17 July 1994|Biarritz, Pyrénée...|   Biarritz|    NULL|    FRA|\n",
       "|Competed in Olymp...|Male|Jacques Marie Sta...|     Jacques•Brugnon|11 May 1895 in Pa...|20 March 1978 in ...|France|         4|168 cm / 64 kg|Sporting club de ...|                Toto|    NULL|       NULL|       NULL|         NULL|      NULL|     Jacques Brugnon|      168|       64|     1895|      1978|     11 May 1895| 20 March 1978|Paris VIIIe, Pari...|Paris VIIIe|   Paris|    FRA|\n",
       "|Competed in Olymp...|Male|  Henry Albert•Canet|        Albert•Canet|17 April 1878 in ...|25 July 1930 in P...|France|         5|          NULL|    TCP, Paris (FRA)|                NULL|    NULL|       NULL|       NULL|         NULL|      NULL|        Albert Canet|     NULL|     NULL|     1878|      1930|   17 April 1878|  25 July 1930|Wandsworth, Engla...| Wandsworth| England|    GBR|\n",
       "+--------------------+----+--------------------+--------------------+--------------------+--------------------+------+----------+--------------+--------------------+--------------------+--------+-----------+-----------+-------------+----------+--------------------+---------+---------+---------+----------+----------------+--------------+--------------------+-----------+--------+-------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_athletes = athletes\n",
    "\n",
    "df_athletes = (\n",
    "    df_athletes\n",
    "    .withColumn('Name', F.regexp_replace('Used name', '•', ' '))\n",
    "    .withColumn('Height_cm', F.regexp_extract('Measurements', r'(\\d+)\\scm', 1).try_cast('int'))\n",
    "    .withColumn('Weight_kg', F.regexp_extract('Measurements', r'(\\d+)\\skg', 1).try_cast('int'))\n",
    "    \n",
    "    # Extracting years as integers\n",
    "    .withColumn('Born_year', F.regexp_extract('Born', r'(\\d{4})', 1).try_cast('int'))\n",
    "    .withColumn('Death_year', F.regexp_extract('Died', r'(\\d{4})', 1).try_cast('int'))\n",
    "    \n",
    "    # Fixed date pattern for \"12 July 1995\"\n",
    "    .withColumn('Born_date', F.regexp_extract('Born', r'(\\d+\\s\\w+\\s\\d{4})', 1))\n",
    "    .withColumn('Death_date', F.regexp_extract('Died', r'(\\d+\\s\\w+\\s\\d{4})', 1))\n",
    "    # Fixed typo: Birth_location\n",
    "    .withColumn('Birth_location', F.regexp_extract('Born', r'in\\s(.*)', 1))\n",
    ")\n",
    "\n",
    "location_map = {\n",
    "    'City': r'^([\\w\\s]+),',\n",
    "    'Region': r',\\s([\\w\\s]+)\\s\\(',\n",
    "    'Country': r'\\((\\w+)\\)',\n",
    "}\n",
    "\n",
    "for col, pattern in location_map.items():\n",
    "    df_athletes = df_athletes.withColumn(\n",
    "        col, F.nullif(F.regexp_extract('Birth_location', pattern, 1), F.lit(\"\"))\n",
    "    )\n",
    "    \n",
    "df_athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a864aec",
   "metadata": {},
   "source": [
    "Convert to dates to to_date dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a11663bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes = (\n",
    "    df_athletes\n",
    "    .withColumn('Born_date', F.try_to_date(F.col('Born_date'), 'd-MMMM-yyyy'))\n",
    "    .withColumn('Death_date', F.try_to_date(F.col('Death_date'), 'd-MMMM-yyyy'))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9a44b",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "925ff17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_athletes = df_athletes.withColumn(\n",
    "    'Age',\n",
    "    F.col('Death_year') - F.col('Born_year')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8255705a",
   "metadata": {},
   "source": [
    "Drop columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0b2a2d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Sex</th><th>NOC</th><th>athlete_id</th><th>Name</th><th>Height_cm</th><th>Weight_kg</th><th>Born_year</th><th>Death_year</th><th>Born_date</th><th>Death_date</th><th>City</th><th>Region</th><th>Country</th><th>Age</th></tr>\n",
       "<tr><td>Male</td><td>France</td><td>1</td><td>Jean-Fran&ccedil;ois Bla...</td><td>NULL</td><td>NULL</td><td>1886</td><td>1960</td><td>NULL</td><td>NULL</td><td>Bordeaux</td><td>Gironde</td><td>FRA</td><td>74</td></tr>\n",
       "<tr><td>Male</td><td>France</td><td>2</td><td>Arnaud Boetsch</td><td>183</td><td>76</td><td>1969</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Meulan</td><td>Yvelines</td><td>FRA</td><td>NULL</td></tr>\n",
       "<tr><td>Male</td><td>France</td><td>3</td><td>Jean Borotra</td><td>183</td><td>76</td><td>1898</td><td>1994</td><td>NULL</td><td>NULL</td><td>Biarritz</td><td>NULL</td><td>FRA</td><td>96</td></tr>\n",
       "<tr><td>Male</td><td>France</td><td>4</td><td>Jacques Brugnon</td><td>168</td><td>64</td><td>1895</td><td>1978</td><td>NULL</td><td>NULL</td><td>Paris VIIIe</td><td>Paris</td><td>FRA</td><td>83</td></tr>\n",
       "<tr><td>Male</td><td>France</td><td>5</td><td>Albert Canet</td><td>NULL</td><td>NULL</td><td>1878</td><td>1930</td><td>NULL</td><td>NULL</td><td>Wandsworth</td><td>England</td><td>GBR</td><td>52</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----+------+----------+--------------------+---------+---------+---------+----------+---------+----------+-----------+--------+-------+----+\n",
       "| Sex|   NOC|athlete_id|                Name|Height_cm|Weight_kg|Born_year|Death_year|Born_date|Death_date|       City|  Region|Country| Age|\n",
       "+----+------+----------+--------------------+---------+---------+---------+----------+---------+----------+-----------+--------+-------+----+\n",
       "|Male|France|         1|Jean-François Bla...|     NULL|     NULL|     1886|      1960|     NULL|      NULL|   Bordeaux| Gironde|    FRA|  74|\n",
       "|Male|France|         2|      Arnaud Boetsch|      183|       76|     1969|      NULL|     NULL|      NULL|     Meulan|Yvelines|    FRA|NULL|\n",
       "|Male|France|         3|        Jean Borotra|      183|       76|     1898|      1994|     NULL|      NULL|   Biarritz|    NULL|    FRA|  96|\n",
       "|Male|France|         4|     Jacques Brugnon|      168|       64|     1895|      1978|     NULL|      NULL|Paris VIIIe|   Paris|    FRA|  83|\n",
       "|Male|France|         5|        Albert Canet|     NULL|     NULL|     1878|      1930|     NULL|      NULL| Wandsworth| England|    GBR|  52|\n",
       "+----+------+----------+--------------------+---------+---------+---------+----------+---------+----------+-----------+--------+-------+----+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_athletes = df_athletes.drop(\n",
    "    'Roles', \n",
    "    'Full name', \n",
    "    'Used name', \n",
    "    'Born', \n",
    "    'Died', \n",
    "    'Measurements', \n",
    "    'Affiliations', \n",
    "    'Nick/petnames', \n",
    "    'Title(s)', \n",
    "    'Other names', \n",
    "    'Nationality', \n",
    "    'Original name', \n",
    "    'Name order',\n",
    "    'Birth_location'\n",
    ")\n",
    "\n",
    "df_athletes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dab520",
   "metadata": {},
   "source": [
    "Will use athlete_id ot merge. Check if all values are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3b92612e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Sex</th><th>NOC</th><th>athlete_id</th><th>Name</th><th>Height_cm</th><th>Weight_kg</th><th>Born_year</th><th>Death_year</th><th>Born_date</th><th>Death_date</th><th>City</th><th>Region</th><th>Country</th><th>Age</th></tr>\n",
       "<tr><td>Male</td><td>13 February 2015 ...</td><td>Canada</td><td> III&quot;</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1934</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "<tr><td>Male</td><td>5 March 1969 in T...</td><td>Netherlands</td><td> Jr.&quot;</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1887</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "<tr><td>Male</td><td>NULL</td><td>United States</td><td> Jr.&quot;</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1962</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "<tr><td>Female</td><td>21 December 1954 ...</td><td>NULL</td><td> -Mill</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "<tr><td>Female</td><td>NULL</td><td>United States</td><td> -Lazenby)&quot;</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1962</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+------+--------------------+-------------+-----------+---------+---------+---------+----------+---------+----------+----+------+-------+----+\n",
       "|   Sex|                 NOC|   athlete_id|       Name|Height_cm|Weight_kg|Born_year|Death_year|Born_date|Death_date|City|Region|Country| Age|\n",
       "+------+--------------------+-------------+-----------+---------+---------+---------+----------+---------+----------+----+------+-------+----+\n",
       "|  Male|13 February 2015 ...|       Canada|       III\"|     NULL|     NULL|     NULL|      1934|     NULL|      NULL|NULL|  NULL|   NULL|NULL|\n",
       "|  Male|5 March 1969 in T...|  Netherlands|       Jr.\"|     NULL|     NULL|     NULL|      1887|     NULL|      NULL|NULL|  NULL|   NULL|NULL|\n",
       "|  Male|                NULL|United States|       Jr.\"|     NULL|     NULL|     NULL|      1962|     NULL|      NULL|NULL|  NULL|   NULL|NULL|\n",
       "|Female|21 December 1954 ...|         NULL|      -Mill|     NULL|     NULL|     NULL|      NULL|     NULL|      NULL|NULL|  NULL|   NULL|NULL|\n",
       "|Female|                NULL|United States| -Lazenby)\"|     NULL|     NULL|     NULL|      1962|     NULL|      NULL|NULL|  NULL|   NULL|NULL|\n",
       "+------+--------------------+-------------+-----------+---------+---------+---------+----------+---------+----------+----+------+-------+----+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter all non integer values\n",
    "df_malformed_id = df_athletes.filter(F.col('athlete_id').try_cast('int').isNull())\n",
    "\n",
    "df_athletes = df_athletes.filter(F.col('athlete_id').try_cast('int').isNotNull())\n",
    "# Check\n",
    "df_malformed_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1fe6ba",
   "metadata": {},
   "source": [
    "## Uplaod to hadoop\n",
    "- Save the malformed IDs to analyze later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "76485568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_athletes.write.mode('overwrite').parquet(\"hdfs:///data/clean/athletes\")\n",
    "df_malformed_id.write.mode('overwrite').parquet(\"hdfs:///data/quarantine\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e4ac3c5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Sex</th><th>NOC</th><th>athlete_id</th><th>Name</th><th>Height_cm</th><th>Weight_kg</th><th>Born_year</th><th>Death_year</th><th>Born_date</th><th>Death_date</th><th>City</th><th>Region</th><th>Country</th><th>Age</th></tr>\n",
       "<tr><td>Male</td><td>Switzerland</td><td>87293</td><td>Henry H&ouml;hnes</td><td>NULL</td><td>NULL</td><td>1889</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td><td>NULL</td></tr>\n",
       "<tr><td>Male</td><td>Czechoslovakia</td><td>87294</td><td>Rudolf H&ouml;hnl</td><td>158</td><td>71</td><td>1946</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Lomazice</td><td>NULL</td><td>CZE</td><td>NULL</td></tr>\n",
       "<tr><td>Male</td><td>Austria</td><td>87295</td><td>Gregor H&ouml;ll</td><td>165</td><td>NULL</td><td>1911</td><td>1999</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Salzburg</td><td>AUT</td><td>88</td></tr>\n",
       "<tr><td>Male</td><td>Austria</td><td>87296</td><td>Rudolf H&ouml;ll</td><td>NULL</td><td>NULL</td><td>1911</td><td>1984</td><td>NULL</td><td>NULL</td><td>Kraubath an der Mur</td><td>Steiermark</td><td>AUT</td><td>73</td></tr>\n",
       "<tr><td>Male</td><td>West Germany</td><td>87297</td><td>Stefan H&ouml;lzlwimmer</td><td>174</td><td>86</td><td>1951</td><td>NULL</td><td>NULL</td><td>NULL</td><td>Salzberg</td><td>Bayern</td><td>GER</td><td>NULL</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----+--------------+----------+------------------+---------+---------+---------+----------+---------+----------+-------------------+----------+-------+----+\n",
       "| Sex|           NOC|athlete_id|              Name|Height_cm|Weight_kg|Born_year|Death_year|Born_date|Death_date|               City|    Region|Country| Age|\n",
       "+----+--------------+----------+------------------+---------+---------+---------+----------+---------+----------+-------------------+----------+-------+----+\n",
       "|Male|   Switzerland|     87293|      Henry Höhnes|     NULL|     NULL|     1889|      NULL|     NULL|      NULL|               NULL|      NULL|   NULL|NULL|\n",
       "|Male|Czechoslovakia|     87294|      Rudolf Höhnl|      158|       71|     1946|      NULL|     NULL|      NULL|           Lomazice|      NULL|    CZE|NULL|\n",
       "|Male|       Austria|     87295|       Gregor Höll|      165|     NULL|     1911|      1999|     NULL|      NULL|               NULL|  Salzburg|    AUT|  88|\n",
       "|Male|       Austria|     87296|       Rudolf Höll|     NULL|     NULL|     1911|      1984|     NULL|      NULL|Kraubath an der Mur|Steiermark|    AUT|  73|\n",
       "|Male|  West Germany|     87297|Stefan Hölzlwimmer|      174|       86|     1951|      NULL|     NULL|      NULL|           Salzberg|    Bayern|    GER|NULL|\n",
       "+----+--------------+----------+------------------+---------+---------+---------+----------+---------+----------+-------------------+----------+-------+----+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_athletes_clean = spark.read.parquet(\"hdfs:///data/clean/athletes\")\n",
    "df_athletes_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89747e0",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "00f2d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = results\n",
    "\n",
    "df_results = (\n",
    "    df_results\n",
    "    .withColumn('Position', F.trim(F.regexp_replace('Pos', '=', ' ')).try_cast('int'))\n",
    "    .withColumn('Games_year', F.regexp_extract('Games', r'(\\d{4})', 1).try_cast('int'))\n",
    "    .withColumn('Season', F.nullif(F.regexp_extract('Games', r'\\b(Summer|Winter|Fall|Spring)\\b', 1), F.lit('')))\n",
    "    .withColumn('Gender', F.nullif(F.regexp_extract('Event', r'\\b(Men|Women)\\b', 1), F.lit(\"\")))\n",
    "    .withColumn('Discipline_clean', F.regexp_replace('Discipline', r'\\s\\(.*\\)', 1))\n",
    "    .withColumn('Name', F.regexp_replace('As', '-', ' '))\n",
    "    .withColumn('Event_clean', F.regexp_extract('Event', r'(.*), ', 1))\n",
    ")\n",
    "\n",
    "cols = ['Season', 'Discipline_clean', 'Event_clean']\n",
    "\n",
    "for c in cols:\n",
    "    df_results = df_results.withColumn(\n",
    "        c,\n",
    "        F.when(F.col(c) == \"\", None).otherwise(F.col(c))\n",
    "    ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "39640b0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Games</th><th>Event</th><th>Team</th><th>Pos</th><th>Medal</th><th>As</th><th>athlete_id</th><th>NOC</th><th>Discipline</th><th>Nationality</th><th>Unnamed: 7</th><th>Position</th><th>Games_year</th><th>Season</th><th>Gender</th><th>Discipline_clean</th><th>Name</th><th>Event_clean</th><th>Points</th><th>Preformance_result</th></tr>\n",
       "<tr><td>1912 Summer Olympics</td><td>Singles, Men (Oly...</td><td>NULL</td><td>=17</td><td>NULL</td><td>Jean-Fran&ccedil;ois Bla...</td><td>1</td><td>FRA</td><td>Tennis</td><td>NULL</td><td>NULL</td><td>17</td><td>1912</td><td>Summer</td><td>Men</td><td>Tennis</td><td>Jean Fran&ccedil;ois Bla...</td><td>Singles</td><td>0</td><td>non-medalist</td></tr>\n",
       "<tr><td>1912 Summer Olympics</td><td>Doubles, Men (Oly...</td><td>Jean Montariol</td><td>DNS</td><td>NULL</td><td>Jean-Fran&ccedil;ois Bla...</td><td>1</td><td>FRA</td><td>Tennis</td><td>NULL</td><td>NULL</td><td>NULL</td><td>1912</td><td>Summer</td><td>Men</td><td>Tennis</td><td>Jean Fran&ccedil;ois Bla...</td><td>Doubles</td><td>0</td><td>non-medalist</td></tr>\n",
       "<tr><td>1920 Summer Olympics</td><td>Singles, Men (Oly...</td><td>NULL</td><td>=32</td><td>NULL</td><td>Jean-Fran&ccedil;ois Bla...</td><td>1</td><td>FRA</td><td>Tennis</td><td>NULL</td><td>NULL</td><td>32</td><td>1920</td><td>Summer</td><td>Men</td><td>Tennis</td><td>Jean Fran&ccedil;ois Bla...</td><td>Singles</td><td>0</td><td>non-medalist</td></tr>\n",
       "<tr><td>1920 Summer Olympics</td><td>Doubles, Mixed (O...</td><td>Jeanne Vaussard</td><td>=8</td><td>NULL</td><td>Jean-Fran&ccedil;ois Bla...</td><td>1</td><td>FRA</td><td>Tennis</td><td>NULL</td><td>NULL</td><td>8</td><td>1920</td><td>Summer</td><td>NULL</td><td>Tennis</td><td>Jean Fran&ccedil;ois Bla...</td><td>Doubles</td><td>0</td><td>non-medalist</td></tr>\n",
       "<tr><td>1920 Summer Olympics</td><td>Doubles, Men (Oly...</td><td>Jacques Brugnon</td><td>4</td><td>NULL</td><td>Jean-Fran&ccedil;ois Bla...</td><td>1</td><td>FRA</td><td>Tennis</td><td>NULL</td><td>NULL</td><td>4</td><td>1920</td><td>Summer</td><td>Men</td><td>Tennis</td><td>Jean Fran&ccedil;ois Bla...</td><td>Doubles</td><td>0</td><td>non-medalist</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+---------------+---+-----+--------------------+----------+---+----------+-----------+----------+--------+----------+------+------+----------------+--------------------+-----------+------+------------------+\n",
       "|               Games|               Event|           Team|Pos|Medal|                  As|athlete_id|NOC|Discipline|Nationality|Unnamed: 7|Position|Games_year|Season|Gender|Discipline_clean|                Name|Event_clean|Points|Preformance_result|\n",
       "+--------------------+--------------------+---------------+---+-----+--------------------+----------+---+----------+-----------+----------+--------+----------+------+------+----------------+--------------------+-----------+------+------------------+\n",
       "|1912 Summer Olympics|Singles, Men (Oly...|           NULL|=17| NULL|Jean-François Bla...|         1|FRA|    Tennis|       NULL|      NULL|      17|      1912|Summer|   Men|          Tennis|Jean François Bla...|    Singles|     0|      non-medalist|\n",
       "|1912 Summer Olympics|Doubles, Men (Oly...| Jean Montariol|DNS| NULL|Jean-François Bla...|         1|FRA|    Tennis|       NULL|      NULL|    NULL|      1912|Summer|   Men|          Tennis|Jean François Bla...|    Doubles|     0|      non-medalist|\n",
       "|1920 Summer Olympics|Singles, Men (Oly...|           NULL|=32| NULL|Jean-François Bla...|         1|FRA|    Tennis|       NULL|      NULL|      32|      1920|Summer|   Men|          Tennis|Jean François Bla...|    Singles|     0|      non-medalist|\n",
       "|1920 Summer Olympics|Doubles, Mixed (O...|Jeanne Vaussard| =8| NULL|Jean-François Bla...|         1|FRA|    Tennis|       NULL|      NULL|       8|      1920|Summer|  NULL|          Tennis|Jean François Bla...|    Doubles|     0|      non-medalist|\n",
       "|1920 Summer Olympics|Doubles, Men (Oly...|Jacques Brugnon|  4| NULL|Jean-François Bla...|         1|FRA|    Tennis|       NULL|      NULL|       4|      1920|Summer|   Men|          Tennis|Jean François Bla...|    Doubles|     0|      non-medalist|\n",
       "+--------------------+--------------------+---------------+---+-----+--------------------+----------+---+----------+-----------+----------+--------+----------+------+------+----------------+--------------------+-----------+------+------------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results = df_results.withColumn('Medal', F.lower(F.trim(F.col('Medal'))))\n",
    "\n",
    "df_results = df_results.withColumn(\n",
    "    'Points',\n",
    "    F.when(F.col('Medal') == 'gold', 3)\n",
    "     .when(F.col('Medal') == 'silver', 2)\n",
    "     .when(F.col('Medal') == 'bronze', 1)\n",
    "     .otherwise(0).cast('bigint') \n",
    ").withColumn(\n",
    "    'Preformance_result',\n",
    "    F.when(F.col('Points') > 0, 'Medalist').otherwise('non-medalist')\n",
    ")\n",
    "\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "416ddc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = df_results.select(\n",
    "    'athlete_id', 'Name', 'Gender', 'Discipline_clean', 'Event_clean', 'Medal', 'Points', 'Preformance_Result', 'Position', 'Games_Year', 'Season'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f72a2c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 18:30:09 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_results.write.mode('overwrite').parquet(\"hdfs:///data/clean/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "83b0fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_clean = spark.read.parquet(\"hdfs:///data/clean/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8e801997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>athlete_id</th><th>Name</th><th>Gender</th><th>Discipline_clean</th><th>Event_clean</th><th>Medal</th><th>Points</th><th>Preformance_Result</th><th>Position</th><th>Games_Year</th><th>Season</th></tr>\n",
       "<tr><td>120662</td><td>BJ Lawrence</td><td>Men</td><td>Athletics</td><td>100 metres</td><td>NULL</td><td>0</td><td>non-medalist</td><td>NULL</td><td>2016</td><td>Summer</td></tr>\n",
       "<tr><td>120663</td><td>Robert Lindstedt</td><td>NULL</td><td>Tennis</td><td>Doubles</td><td>NULL</td><td>0</td><td>non-medalist</td><td>9</td><td>2012</td><td>Summer</td></tr>\n",
       "<tr><td>120663</td><td>Robert Lindstedt</td><td>Men</td><td>Tennis</td><td>Doubles</td><td>NULL</td><td>0</td><td>non-medalist</td><td>9</td><td>2012</td><td>Summer</td></tr>\n",
       "<tr><td>120664</td><td>Mariya Baklakova</td><td>Women</td><td>Swimming1</td><td>4 &times; 200 metres Fr...</td><td>NULL</td><td>0</td><td>non-medalist</td><td>NULL</td><td>2012</td><td>Summer</td></tr>\n",
       "<tr><td>120665</td><td>Mariya Gromova</td><td>Women</td><td>Swimming1</td><td>4 &times; 100 metres Me...</td><td>NULL</td><td>0</td><td>non-medalist</td><td>NULL</td><td>2012</td><td>Summer</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------+----------------+------+----------------+--------------------+-----+------+------------------+--------+----------+------+\n",
       "|athlete_id|            Name|Gender|Discipline_clean|         Event_clean|Medal|Points|Preformance_Result|Position|Games_Year|Season|\n",
       "+----------+----------------+------+----------------+--------------------+-----+------+------------------+--------+----------+------+\n",
       "|    120662|     BJ Lawrence|   Men|       Athletics|          100 metres| NULL|     0|      non-medalist|    NULL|      2016|Summer|\n",
       "|    120663|Robert Lindstedt|  NULL|          Tennis|             Doubles| NULL|     0|      non-medalist|       9|      2012|Summer|\n",
       "|    120663|Robert Lindstedt|   Men|          Tennis|             Doubles| NULL|     0|      non-medalist|       9|      2012|Summer|\n",
       "|    120664|Mariya Baklakova| Women|       Swimming1|4 × 200 metres Fr...| NULL|     0|      non-medalist|    NULL|      2012|Summer|\n",
       "|    120665|  Mariya Gromova| Women|       Swimming1|4 × 100 metres Me...| NULL|     0|      non-medalist|    NULL|      2012|Summer|\n",
       "+----------+----------------+------+----------------+--------------------+-----+------+------------------+--------+----------+------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24b0cbe",
   "metadata": {},
   "source": [
    "## Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "37b7c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['athlete_id','height_cm', 'weight_kg', 'Born_year', 'Death_year', 'Country']\n",
    "\n",
    "df_merge = df_results_clean.join(df_athletes_clean.select(columns), on='athlete_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8d37120b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "df_merge = df_merge.withColumn(\n",
    "    'Age',\n",
    "    (F.col('Death_year') - F.col('Born_year')).try_cast('int')\n",
    ")\n",
    "\n",
    "splits = [-float(\"inf\"), 13, 20, 30, 40, 50, 60, 70, 80, float(\"inf\")]\n",
    "labels = {0.0: \"11-12\", \n",
    "          1.0: \"13-19\", \n",
    "          2.0: \"20-29\", \n",
    "          3.0: \"30-39\", \n",
    "          4.0: \"40-49\", \n",
    "          5.0: \"50-59\", \n",
    "          6.0: \"60-69\", \n",
    "          7.0: \"70-79\", \n",
    "          8.0: \"80+\"\n",
    "          }\n",
    "\n",
    "# Apply bucketizer\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"Age\", outputCol=\"Age_idx\", handleInvalid=\"keep\")\n",
    "df_merge = bucketizer.transform(df_merge)\n",
    "\n",
    "map = F.create_map([F.lit(x) for i in labels.items() for x in i])\n",
    "\n",
    "df_merge = df_merge.withColumn(\"Age_group\", map[F.col(\"Age_idx\")])\n",
    "\n",
    "# Set null to unknown\n",
    "df_merge = df_merge.withColumn(\n",
    "    \"Age_group\", \n",
    "    F.when(F.col(\"Age\").isNull(), \"Unknown\").otherwise(F.col(\"Age_group\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c826b",
   "metadata": {},
   "source": [
    "Calculate BMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1b5ba4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 18:54:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:322)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "26/02/21 18:54:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:322)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "df_merge = df_merge.withColumn(\n",
    "    'BMI',\n",
    "    F.round((F.col('Weight_kg') / (F.col('Height_cm') / 100)*2).try_cast('double'), 2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc08468",
   "metadata": {},
   "source": [
    "Olympic Games Year Total Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9cc1d99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year_points = df_merge.groupBy('Games_year', 'Age_group').agg(\n",
    "    F.sum('Points').alias('Total_points')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9955de6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Games_year</th><th>Age_group</th><th>Total_points</th></tr>\n",
       "<tr><td>2016</td><td>Unknown</td><td>4134</td></tr>\n",
       "<tr><td>1906</td><td>60-69</td><td>147</td></tr>\n",
       "<tr><td>1900</td><td>Unknown</td><td>220</td></tr>\n",
       "<tr><td>1924</td><td>30-39</td><td>31</td></tr>\n",
       "<tr><td>2010</td><td>20-29</td><td>3</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------+---------+------------+\n",
       "|Games_year|Age_group|Total_points|\n",
       "+----------+---------+------------+\n",
       "|      2016|  Unknown|        4134|\n",
       "|      1906|    60-69|         147|\n",
       "|      1900|  Unknown|         220|\n",
       "|      1924|    30-39|          31|\n",
       "|      2010|    20-29|           3|\n",
       "+----------+---------+------------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_year_points.write.mode('overwrite').parquet(\"hdfs:///data/clean/total_year_points\")\n",
    "df_year_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60f05b",
   "metadata": {},
   "source": [
    "Find podium appearances percentage of the age groups and their disicipline\n",
    "- Because the age group 20-29 has more participants they will always have more points so here calculate podium appearance percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "623a9dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Games_year</th><th>Age_group</th><th>Discipline_clean</th><th>Total_athletes</th><th>Podium_appearance</th><th>%</th></tr>\n",
       "<tr><td>1988</td><td>Unknown</td><td>Rowing</td><td>594</td><td>157</td><td>26.43</td></tr>\n",
       "<tr><td>1996</td><td>Unknown</td><td>Football1</td><td>385</td><td>97</td><td>25.19</td></tr>\n",
       "<tr><td>2020</td><td>Unknown</td><td>Softball1</td><td>90</td><td>45</td><td>50.0</td></tr>\n",
       "<tr><td>1906</td><td>60-69</td><td>Diving1</td><td>22</td><td>3</td><td>13.64</td></tr>\n",
       "<tr><td>2020</td><td>Unknown</td><td>Wrestling</td><td>271</td><td>69</td><td>25.46</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------+---------+----------------+--------------+-----------------+-----+\n",
       "|Games_year|Age_group|Discipline_clean|Total_athletes|Podium_appearance|    %|\n",
       "+----------+---------+----------------+--------------+-----------------+-----+\n",
       "|      1988|  Unknown|          Rowing|           594|              157|26.43|\n",
       "|      1996|  Unknown|       Football1|           385|               97|25.19|\n",
       "|      2020|  Unknown|       Softball1|            90|               45| 50.0|\n",
       "|      1906|    60-69|         Diving1|            22|                3|13.64|\n",
       "|      2020|  Unknown|       Wrestling|           271|               69|25.46|\n",
       "+----------+---------+----------------+--------------+-----------------+-----+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_podium_appearance_age = df_merge.groupBy('Games_year', 'Age_group', 'Discipline_clean').agg(\n",
    "    F.count('athlete_id').alias('Total_athletes'),\n",
    "    F.count('Medal').alias('Podium_appearance')\n",
    ")\n",
    "\n",
    "df_podium_appearance_age = df_podium_appearance_age.withColumn(\n",
    "    '%',\n",
    "    F.round((F.col('Podium_appearance') / F.col('Total_athletes'))*100, 2)\n",
    ")\n",
    "df_podium_appearance_age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ee65b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_podium_appearance_age.write.mode('overwrite').parquet(\"hdfs:///data/clean/podium_appearance_age_%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65afeb04",
   "metadata": {},
   "source": [
    "The mean & std of medalist vs non-medalist in their discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5d7fb88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Games_year</th><th>Discipline_clean</th><th>Preformance_result</th><th>Height_mean</th><th>Height_std</th><th>Weight_mean</th><th>Weight_std</th></tr>\n",
       "<tr><td>2020</td><td>Marathon Swimming1</td><td>non-medalist</td><td>174.17</td><td>9.79</td><td>65.23</td><td>10.77</td></tr>\n",
       "<tr><td>2018</td><td>Short Track Speed...</td><td>non-medalist</td><td>170.97</td><td>7.97</td><td>64.55</td><td>8.7</td></tr>\n",
       "<tr><td>2020</td><td>Biathlon</td><td>non-medalist</td><td>170.21</td><td>8.93</td><td>57.55</td><td>8.2</td></tr>\n",
       "<tr><td>2018</td><td>Diving1</td><td>non-medalist</td><td>174.0</td><td>0.0</td><td>63.0</td><td>0.0</td></tr>\n",
       "<tr><td>2020</td><td>Freestyle Skiing1</td><td>non-medalist</td><td>174.87</td><td>7.21</td><td>66.46</td><td>8.95</td></tr>\n",
       "</table>\n",
       "only showing top 5 rows\n"
      ],
      "text/plain": [
       "+----------+--------------------+------------------+-----------+----------+-----------+----------+\n",
       "|Games_year|    Discipline_clean|Preformance_result|Height_mean|Height_std|Weight_mean|Weight_std|\n",
       "+----------+--------------------+------------------+-----------+----------+-----------+----------+\n",
       "|      2020|  Marathon Swimming1|      non-medalist|     174.17|      9.79|      65.23|     10.77|\n",
       "|      2018|Short Track Speed...|      non-medalist|     170.97|      7.97|      64.55|       8.7|\n",
       "|      2020|            Biathlon|      non-medalist|     170.21|      8.93|      57.55|       8.2|\n",
       "|      2018|             Diving1|      non-medalist|      174.0|       0.0|       63.0|       0.0|\n",
       "|      2020|   Freestyle Skiing1|      non-medalist|     174.87|      7.21|      66.46|      8.95|\n",
       "+----------+--------------------+------------------+-----------+----------+-----------+----------+\n",
       "only showing top 5 rows"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 19:06:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/02/21 19:06:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/02/21 19:07:54 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/02/21 19:07:54 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/02/21 19:09:35 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/02/21 19:09:35 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df_physical_preformance = df_merge.groupBy('Games_year', 'Discipline_clean', 'Preformance_result').agg(\n",
    "    F.round(F.mean('Height_cm'), 2).alias('Height_mean'),\n",
    "    F.round(F.stddev('Height_cm'), 2).alias('Height_std'),\n",
    "    F.round(F.mean('Weight_kg'), 2).alias('Weight_mean'),\n",
    "    F.round(F.stddev('Weight_kg'), 2).alias('Weight_std')\n",
    ")\n",
    "\n",
    "df_physical_preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a6eefc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/21 19:12:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/02/21 19:12:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.20.10.2:59934\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df_physical_preformance.write.mode('overwrite').parquet(\"hdfs:///data/clean/physical_preformance_athlete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
